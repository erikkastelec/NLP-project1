{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import random\n",
    "import signal\n",
    "\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import progressbar\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from venn import venn\n",
    "from networkx.exception import NetworkXNoPath, NodeNotFound\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from build_graph import build_graph\n",
    "from helper_functions import read_pickle, get_link_from_article, write_pickle, ner_extract_all, clean_all_ner\n",
    "from recommendation import get_article_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    articles = read_pickle('./analysis.pickle')\n",
    "    graph = read_pickle('./graph.pickle')\n",
    "    try:\n",
    "        node_to_article_map = read_pickle('./node_to_article_map.pickle')\n",
    "        article_dict = read_pickle('./article_dict.pickle')\n",
    "    except FileNotFoundError:\n",
    "        node_to_article_map = {}\n",
    "        article_dict = {}\n",
    "        for i in range(0, len(articles)):\n",
    "            article_dict[get_link_from_article(\"RTVSLO\", articles[i])] = articles[i]\n",
    "            for per in articles[i][\"per\"]:\n",
    "                try:\n",
    "                    node_to_article_map[per[0]].append(i)\n",
    "                except KeyError:\n",
    "                    node_to_article_map[per[0]] = [i]\n",
    "            for org in articles[i][\"org\"]:\n",
    "                try:\n",
    "                    node_to_article_map[org[0]].append(i)\n",
    "                except KeyError:\n",
    "                    node_to_article_map[org[0]] = [i]\n",
    "        write_pickle(article_dict, './article_dict.pickle')\n",
    "        write_pickle(node_to_article_map, './node_to_article_map.pickle')\n",
    "except FileNotFoundError:\n",
    "    print(\"err\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "    dej = 0\n",
    "    x = article_dict[\"https://www.rtvslo.si/moja-generacija/80-letna-sophia-loren-gleda-na-zivljenje-pozitivno/346813\"]\n",
    "    y = article_dict[\"https://www.rtvslo.si/zabava-in-slog/znani/foto-80-let-sophie-loren-najboljsega-italijanskega-izvoza-po-pasti/346712\"]\n",
    "    x = ([f[0] for f in x[\"per\"]] + [f[0] for f in x[\"org\"]], x[\"name\"])\n",
    "    y = ([f[0] for f in y[\"per\"]] + [f[0] for f in y[\"org\"]], y[\"name\"])\n",
    "    subgraph = nx.Graph()\n",
    "    nodes1 = x[0]\n",
    "    nodes2 = y[0]\n",
    "    nodes = nodes1 + nodes2\n",
    "    visual = nx.Graph()\n",
    "    f = 0\n",
    "    g = 1\n",
    "    if dej == 0:\n",
    "        visual.add_node(nodes1[f], layer=1)\n",
    "        visual.add_node(nodes2[g], layer=3)\n",
    "        a = set(nx.neighbors(graph, nodes1[f]))\n",
    "        b = set(nx.neighbors(graph, nodes2[g]))\n",
    "        for x in a.intersection(b):\n",
    "            visual.add_node(x, layer=2)\n",
    "            visual.add_edge(nodes2[g], x)\n",
    "            visual.add_edge(nodes1[f], x)\n",
    "        a.add(nodes1[f])\n",
    "        b.add(nodes2[g])\n",
    "        ba = set()\n",
    "        ba.add(nodes1[f])\n",
    "        bb = set()\n",
    "        bb.add(nodes2[g])\n",
    "        ven_dict = {\n",
    "            \"N_i\": a,\n",
    "            \"N_j\": b,\n",
    "        }\n",
    "        print(ven_dict)\n",
    "        venn(ven_dict)\n",
    "        plt.show()\n",
    "        dej = 1\n",
    "    for i in range(0, len(nodes)):\n",
    "        # Construct a set from neighbor list\n",
    "        a = set(nx.neighbors(graph, nodes[i]))\n",
    "        # for a1 in a.copy():\n",
    "        #     for neig in nx.neighbors(graph,a1):\n",
    "        #         a.add(neig)\n",
    "        count_sum_a = sum([graph[nodes[i]][x][\"count\"] for x in a])\n",
    "        occ_count_a = sum([graph[nodes[i]][x][\"occ\"] for x in a])\n",
    "        for j in range(i + 1, len(nodes)):\n",
    "            if nodes[i] == nodes[j]:\n",
    "                continue\n",
    "            b = set(nx.neighbors(graph, nodes[j]))\n",
    "            count_sum_b = sum([graph[nodes[j]][x][\"count\"] for x in b])\n",
    "            occ_count_b = sum([graph[nodes[j]][x][\"occ\"] for x in b])\n",
    "            try:\n",
    "                # Combined weights\n",
    "                intersection = a.intersection(b)\n",
    "                tmp = 0\n",
    "                for inter in intersection:\n",
    "                    tmp += graph[nodes[i]][inter][\"occ\"] / count_sum_a\n",
    "                    tmp += graph[nodes[j]][inter][\"occ\"] / count_sum_b\n",
    "                cond_prob = max(1 - (tmp / 2.0), 1e-16)\n",
    "                subgraph.add_edge(nodes[i], nodes[j], weight=cond_prob)\n",
    "            except ZeroDivisionError:\n",
    "                continue\n",
    "\n",
    "\n",
    "    plt.plot()\n",
    "    pos = nx.multipartite_layout(visual, subset_key=\"layer\")\n",
    "    nx.draw(visual, pos)\n",
    "    plt.show()\n",
    "    colormap = []\n",
    "\n",
    "    for node in subgraph:\n",
    "        if node in nodes1 and node in nodes2:\n",
    "            colormap.append(\"green\")\n",
    "        elif node in nodes1:\n",
    "            colormap.append(\"blue\")\n",
    "        elif node in nodes2:\n",
    "            colormap.append(\"red\")\n",
    "    plt.plot()\n",
    "    pos = nx.circular_layout(subgraph)\n",
    "    nx.draw(subgraph, pos, node_color=colormap)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    CNREC_PATH = './data/CNRec'\n",
    "    try:\n",
    "        articles = read_pickle(CNREC_PATH + '/analysis_cnrec.pickle')\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            ner_data = read_pickle(CNREC_PATH + '/ner_data_cnrec.pickle')\n",
    "        except FileNotFoundError:\n",
    "            ner_data = ner_extract_all(path=CNREC_PATH + \"/CNRec_RawText\", gpu=True, website=\"other\", language=\"en\")\n",
    "            write_pickle(ner_data, CNREC_PATH + '/ner_data_cnrec.pickle')\n",
    "        articles = clean_all_ner(ner_data)\n",
    "        # Write data to file\n",
    "        write_pickle(articles, CNREC_PATH + '/analysis_cnrec.pickle')\n",
    "\n",
    "    try:\n",
    "        g = read_pickle(CNREC_PATH + '/graph.pickle')\n",
    "    except FileNotFoundError:\n",
    "        g, co = build_graph(articles)\n",
    "        write_pickle(g, CNREC_PATH + '/graph.pickle')\n",
    "\n",
    "    # load article id dictionary\n",
    "    article_mapper = {}\n",
    "    with open(CNREC_PATH + '/articleToID.csv') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for k, v in reader:\n",
    "            article_mapper[v] = k\n",
    "    # list articles to dict\n",
    "    article_dict = {}\n",
    "    for article in articles:\n",
    "        article_dict[article_mapper[article[\"name\"]]] = article\n",
    "    try:\n",
    "        ground_truth = pd.read_pickle(CNREC_PATH + '/ground_truth_069.pickle')\n",
    "    except FileNotFoundError:\n",
    "\n",
    "        ground_truth = pd.read_csv(CNREC_PATH + '/CNRec_groundTruth.csv')\n",
    "        ground_truth[\"ourSim\"] = 0.0\n",
    "\n",
    "        for i, row in ground_truth.iterrows():\n",
    "            x = article_dict[str(round(row[\"art1\"]))]\n",
    "            x = ([f[0] for f in x[\"per\"]] + [f[0] for f in x[\"org\"]], x[\"name\"])\n",
    "            y = article_dict[str(round(row[\"art2\"]))]\n",
    "            y = ([f[0] for f in y[\"per\"]] + [f[0] for f in y[\"org\"]], y[\"name\"])\n",
    "            ground_truth.loc[i, \"ourSim\"] = get_article_similarity(x, y, g)[0]\n",
    "        ground_truth.to_pickle(CNREC_PATH + \"/ground_truth.pickle\")\n",
    "\n",
    "    col1 = ground_truth[\"ourSim\"]\n",
    "    ground_truth[\"meanSimRating\"] = ground_truth[\"meanSimRating\"] / 2\n",
    "    col2 = ground_truth[\"meanSimRating\"]\n",
    "    view = ground_truth[[\"ourSim\", \"meanSimRating\", \"art1\", \"art2\"]]\n",
    "    art1 = 0\n",
    "    art2 = 1\n",
    "    ground_truth_num = 1\n",
    "   # print(\"Ground truth: \", ground_truth[\"meanSimRating\"][ground_truth_num])\n",
    "    x = article_dict[str(art1)]\n",
    "    x = ([f[0] for f in x[\"per\"]] + [f[0] for f in x[\"org\"]], x[\"name\"])\n",
    "    y = article_dict[str(art2)]\n",
    "    y = ([f[0] for f in y[\"per\"]] + [f[0] for f in y[\"org\"]], y[\"name\"])\n",
    "    # print(\"Entities art1: \", x[0])\n",
    "    # print(\"Entities art2: \", y[0])\n",
    "    set_x = set(x[0])\n",
    "    set_y = set(y[0])\n",
    "    intersection = set_x.intersection(set_y)\n",
    "    # print(\"Entities in both: \", intersection)\n",
    "    # print(\"Result: \", get_article_similarity(x, y, g))\n",
    "    ground_truth[\"goodRatingOur\"] = (ground_truth[\"ourSim\"] >= 0.5).astype(int)\n",
    "    ground_truth[\"goodRatingOur75\"] = (ground_truth[\"ourSim\"] >= 0.75).astype(int)\n",
    "    ground_truth[\"goodRatingDiversityOur\"] = (ground_truth[\"ourSim\"] >= 0.5).astype(int)\n",
    "    ground_truth[\"goodRatingDiversityOur75\"] = (ground_truth[\"ourSim\"] >= 0.75).astype(int)\n",
    "    ground_truth[\"correct50\"] = (ground_truth[\"goodRatingOur\"] == ground_truth[\"GoodR+AF8-50\"]).astype(int)\n",
    "    ground_truth[\"correct75\"] = (ground_truth[\"goodRatingOur75\"] == ground_truth[\"GoodR+AF8-75\"]).astype(int)\n",
    "    ground_truth[\"correctDiv50\"] = (ground_truth[\"goodRatingDiversityOur\"] == ground_truth[\"diversity+AF8-50\"]).astype(\n",
    "        int)\n",
    "    ground_truth[\"correctDiv75\"] = (\n",
    "                ground_truth[\"goodRatingDiversityOur75\"] == ground_truth[\"diversity+AF8-75\"]).astype(int)\n",
    "    # print(\"Accurate recommendation GR50: \", sum(ground_truth[\"correct50\"]) / len(ground_truth[\"correct50\"]))\n",
    "    # print(\"Accurate recommendation GR75: \", sum(ground_truth[\"correct75\"]) / len(ground_truth[\"correct75\"]))\n",
    "    # print(\"Accurate recommendation DR50: \", sum(ground_truth[\"correctDiv50\"]) / len(ground_truth[\"correctDiv50\"]))\n",
    "    # print(\"Accurate recommendation DR75: \", sum(ground_truth[\"correctDiv75\"]) / len(ground_truth[\"correctDiv75\"]))\n",
    "    #\n",
    "    # print(\"Pearson correlation: \", col1.corr(col2))\n",
    "    #\n",
    "    # print(\"Spearman correlation: \", col1.corr(col2, method=\"spearman\"))\n",
    "\n",
    "\n",
    "\n",
    "    def get_preds(threshold, probabilities):\n",
    "        return [1 if prob > threshold else 0 for prob in probabilities]\n",
    "    roc_values_50 = []\n",
    "    roc_values_75 = []\n",
    "    roc_values_d50 = []\n",
    "    roc_values_d75 = []\n",
    "    roc_values_avg = []\n",
    "    max = 0\n",
    "    maxi = 0\n",
    "    for thresh in np.linspace(0, 1, 100):\n",
    "        preds = get_preds(thresh, ground_truth[\"ourSim\"])\n",
    "        tn, fp, fn, tp = confusion_matrix(ground_truth[\"GoodR+AF8-50\"], preds).ravel()\n",
    "        tpr = tp/(tp+fn)\n",
    "        preds = get_preds(thresh, ground_truth[\"ourSim\"])\n",
    "        f1_50 = f1_score(ground_truth[\"GoodR+AF8-50\"], preds)\n",
    "        f1_75 = f1_score(ground_truth[\"GoodR+AF8-75\"], preds)\n",
    "        f1_d50 = f1_score(ground_truth[\"diversity+AF8-50\"], preds)\n",
    "        f1_d75 = f1_score(ground_truth[\"diversity+AF8-75\"], preds)\n",
    "        fpr = fp/(fp+tn)\n",
    "\n",
    "        roc_values_50.append([f1_50, thresh])\n",
    "        roc_values_75.append([f1_75, thresh])\n",
    "        roc_values_d50.append([f1_d50, thresh])\n",
    "        roc_values_d75.append([f1_d75, thresh])\n",
    "        if max < (f1_50 + f1_75)/2:\n",
    "            max = (f1_50 + f1_75)/2\n",
    "            maxi = thresh\n",
    "        roc_values_avg.append([(f1_50 + f1_75 + f1_d75 + f1_d50)/4, thresh])\n",
    "    print(max, \" : \", maxi)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,7))\n",
    "    tpr_values, fpr_values = zip(*roc_values_50)\n",
    "    ax.plot(fpr_values, tpr_values, label=\"GoodR 50\")\n",
    "    tpr_values, fpr_values = zip(*roc_values_75)\n",
    "    ax.plot(fpr_values, tpr_values, label=\"GoodR 75\")\n",
    "    tpr_values, fpr_values = zip(*roc_values_d50)\n",
    "    ax.plot(fpr_values, tpr_values, label=\"Diversity 50\")\n",
    "    tpr_values, fpr_values = zip(*roc_values_d75)\n",
    "    ax.plot(fpr_values, tpr_values, label=\"Diversity 75\")\n",
    "    tpr_values, fpr_values = zip(*roc_values_avg)\n",
    "    ax.axvline(x=0.4848484848, label=\"Optimal threshold\")\n",
    "\n",
    "    # ax.plot(fpr_values, tpr_values, label=\"Average\")\n",
    "    # ax.plot(np.linspace(0, 1, 100),\n",
    "    #          np.linspace(0, 1, 100),\n",
    "    #          label='baseline',\n",
    "    #          linestyle='--')\n",
    "    # plt.title('Receiver Operating Characteristic Curve', fontsize=18)\n",
    "    plt.ylabel('F1 Score', fontsize=16)\n",
    "    plt.xlabel('Prediction threshold', fontsize=16)\n",
    "    plt.legend(fontsize=12)\n",
    "    ground_truth[\"goodRatingOur\"] = (ground_truth[\"ourSim\"] >= 0.6).astype(int)\n",
    "    ground_truth[\"goodRatingOur75\"] = (ground_truth[\"ourSim\"] >= 0.75).astype(int)\n",
    "    ground_truth[\"correct50\"] = (ground_truth[\"goodRatingOur\"] == ground_truth[\"GoodR+AF8-50\"]).astype(int)\n",
    "    print(\"Accurate recommendation GR50 new: \", sum(ground_truth[\"correct50\"]) / len(ground_truth[\"correct50\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.read_csv('./skupen.csv')\n",
    "df.loc[43, \"recommendPerson\"] = 0.0\n",
    "df.loc[43, \"scoreAlgo\"] = 0.0\n",
    "\n",
    "article_dict = read_pickle('./article_dict.pickle')\n",
    "for i, row in df.iterrows():\n",
    "    x = articles[article_dict[row[\"article1\"]]]\n",
    "    x = ([f[0] for f in x[\"per\"]] + [f[0] for f in x[\"org\"]], x[\"name\"])\n",
    "    y = articles[article_dict[row[\"article1\"]]]\n",
    "    y = ([f[0] for f in y[\"per\"]] + [f[0] for f in y[\"org\"]], y[\"name\"])\n",
    "    df.loc[i, \"scoreAlgo\"] = get_article_similarity(x, y, graph)[0]\n",
    "\n",
    "\n",
    "def get_preds(threshold, probabilities):\n",
    "    return [1 if prob > threshold else 0 for prob in probabilities]\n",
    "roc_values = []\n",
    "max = 0\n",
    "maxi = 0\n",
    "for thresh in np.linspace(0, 1, 100):\n",
    "    preds = get_preds(thresh, df[\"scoreAlgo\"])\n",
    "    tn, fp, fn, tp = confusion_matrix( df[\"recommendPerson\"], preds).ravel()\n",
    "    tpr = tp/(tp+fn)\n",
    "    f1_50 = f1_score(df[\"recommendPerson\"], preds)\n",
    "    fpr = fp/(fp+tn)\n",
    "    if max < f1_50:\n",
    "        max = f1_50\n",
    "        maxi = thresh\n",
    "    roc_values.append([f1_50, thresh])\n",
    "\n",
    "print(max,\" : \", maxi)\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "tpr_values, fpr_values = zip(*roc_values)\n",
    "ax.plot(fpr_values, tpr_values, label=\"Good recommendation\")\n",
    "\n",
    "# ax.plot(fpr_values, tpr_values, label=\"Average\")\n",
    "# ax.plot(np.linspace(0, 1, 100),\n",
    "#          np.linspace(0, 1, 100),\n",
    "#          label='baseline',\n",
    "#          linestyle='--')\n",
    "# plt.title('Receiver Operating Characteristic Curve', fontsize=18)\n",
    "plt.ylabel('F1 Score', fontsize=16)\n",
    "plt.xlabel('Prediction threshold', fontsize=16)\n",
    "plt.legend(fontsize=12)\n",
    "   # print(\"Accurate recommendation: \", sum(df[\"correct50\"]) / len(df[\"correct50\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}